
# Oh My Assistant: Generative AI Assistant Service for Comic ariststs

<p align='center'>
    <img src="https://github.com/kylew1004/doraemon_web/assets/20416616/adb0d0a7-3c1d-4edb-9f1e-bebb0deed825" width=500>
</p>

> Oh My Assistant is a generative AI service designed to assist comics and illustration artists by learning each artist's unique drawing style to convert realistic images into their style and translate characters' poses to a desired pose. 

<div align=center>
    <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat&logo=PyTorch&logoColor=white">
    <img src="https://img.shields.io/badge/Python-3776AB?style=flat&logo=Python&logoColor=white">
    <img src="https://img.shields.io/badge/React-61DAFB?style=flat&logo=React&logoColor=white">
    <img src="https://img.shields.io/badge/FastAPI-009688?style=flat&logo=FastAPI&logoColor=white">
    <img src="https://img.shields.io/badge/Docker-2496ED?style=flat&logo=Docker&logoColor=white">
</div>


## Table of content
- [Live Demo](#Demo)
- [Member](#Member)
- [Service Architecture](#Service)
- [Modelling-Background](#ModelBackground)
- [Modeling-Pose](#ModelPose)


## Live Demo <a id = 'Demo'></a>
- The service is deployed on the following [link](http://www.gangyub.site) (As of August 2024, the link may not work).

### Background Image Generator
https://github.com/kylew1004/doraemon_web/assets/20416616/1b37ceaf-63a6-4d03-8aa5-eb76818fd304


### Pose Image Generator


https://github.com/kylew1004/doraemon_web/assets/20416616/59ba6d9f-e47d-4781-a622-0b9e073ec84e



## Member <a id = 'Member'></a>

|Chanwoo Kim|Hyunwoo Nam|Kyungyub Ryu|Gyuseob Lee|Hyeonji Lee|Juhee Han|
|:--:|:--:|:--:|:--:|:--:|:--:|
|<a href='https://github.com/uowol'><img src='https://avatars.githubusercontent.com/u/20416616?v=4' width='100px'/></a>|<a href='https://github.com/nhw2417'><img src='https://avatars.githubusercontent.com/u/103584775?s=88&v=4' width='100px'/></a>|<a href='https://github.com/kylew1004'><img src='https://avatars.githubusercontent.com/u/5775698?s=88&v=4' width='100px'/></a>|<a href='https://github.com/9sub'><img src='https://avatars.githubusercontent.com/u/113101019?s=88&v=4' width='100px'/></a>|<a href='https://github.com/solee328'><img src='https://avatars.githubusercontent.com/u/22787039?s=88&v=4' width='100px'/></a>|<a href='https://github.com/jh7316'><img src='https://avatars.githubusercontent.com/u/95545960?s=88&v=4' width='100px'/></a>|
|Modeling|Modeling|Backend|Backend|Modeling|Frontend|
|Background<br>Image<br>Generate|Background<br>Image<br>Generate|PL<br>Infra<br>Serving|Implement BE|Pose<br>Image<br>Generate|UI/UX Design<br>Implement FE |  

---


## Service Architecture <a id = 'Service'></a>

<p align='center'>
    <img src="https://github.com/kylew1004/doraemon_web/assets/5775698/254a54c9-9f83-4f00-a26d-e73a57ac4bb5" width="80%>
</p>


 - To reduce interdependencies between services, the web server and model server have been separated.
 - The web frontend server, backend server, and database server are all hosted on AWS EC2 servers, while the model server is hosted on an NCP V100 server.
 - The backend server sends input images or prompts received from the frontend to the model server, and the model server responds with the inference results for the given input.
 - The backend server then sends these results back to the frontend to be displayed to the user. If the user wishes to save the results, the frontend sends a request to the backend server to store the relevant images in AWS S3 storage.



## Modeling - Background <a id = 'ModelBackground'></a>

### Inference
<p align='center'>
    <img src="https://github.com/kylew1004/doraemon_web/assets/20416616/360b9289-0469-4a5e-8e01-fc3e5e87ada9" width="80%">
</p>

The background generation service selects either the Img2Img or Txt2Img model of Stable Diffusion depending on whether an source image is provided by the user. Then, it follows these steps to create webtoon-style images.
1. Noise Initialization
    - If an original image is provided, noise is generated by gradually adding noise to the original image. If no original image is given, noise is generated as a completely random tensor.
2. Inject Condition
    - Our model takes prompts as input and uses the text encoder of the CLIP model to extract embedding vectors. 
3. Denoising Process
    - The noise generated earlier is used as input to progressively remove noise and produce a high-resolution image. A cross-attention mechanism is employed to guide the noise removal process according to the given features.

In summary, by inputting an original image or prompt into a model trained in webtoon style, the model generates and removes noise based on the input to produce the background image required by the artist.

### Result
![image](https://github.com/boostcampaitech6/level2-cv-datacentric-cv-02/assets/20416616/5c77ea70-74aa-45bc-8efc-57b0c7f6a3b7)
![image](https://github.com/boostcampaitech6/level2-cv-datacentric-cv-02/assets/20416616/b4b98d35-2718-483c-a20f-86599dfd2b61)


## Modeling - Pose <a id = 'ModelPose'></a>
Character Pose Transfer is divided into 2 steps: Pose Estimation and Pose Transfer.

### Pose Estimation
<p align='center'>
    <img src="https://github.com/kylew1004/doraemon_web/assets/22787039/2a586197-552f-44d6-a441-1b7410aabbbf" width="60%">
</p>

Pose Estimation model uses <a href="https://github.com/IDEA-Research/DWPose/tree/onnx" target="_blank">DWPose</a>. DWPose uses a detector to find landmarks in the input image and a classifier to categorize body keypoints, predicting a total of 133 keypoints (COCO Whole Body) for the face, hands, arms, and legs.


### Pose Transfer
<p align='center'>
    <img src="https://github.com/kylew1004/doraemon_web/assets/22787039/0690d10f-66db-4110-b7ae-f7744ab86aad" width="80%">
</p>

Pose Transfer uses a diffusion model based on <a href="https://github.com/MooreThreads/Moore-AnimateAnyone" target="_blank">AnimateAnyone</a>. The target pose image extracted from the previous estimation and the character image provided by the user are input and embedded using a VAE and CLIP Encoder. Denoising UNet and ReferenceNet are used to generate the pose-changed character image from noise, and the VAE Decoder is employed to decode the image and produce the final result.


## Directory
<!-- 합치면 -->

## Links
- [Presentation Video(Kor)](https://youtu.be/huVYC4bZgOg)
- [Wrapup Reports(Kor)](https://pebble-ziconium-f61.notion.site/Wrap-up-Reports-42bf8884d38244afbb5ef24a6f06ed3e?pvs=4)



